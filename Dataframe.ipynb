{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fda5dd1",
   "metadata": {},
   "source": [
    "# Initialize Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b416997",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df748e8e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Dataframe operations\").master(\"local[3]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387230df",
   "metadata": {},
   "source": [
    "# 1. Create Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331f64ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# create dummy data\n",
    "columns = [\"languages\", \"users_count\"]\n",
    "data = [(\"Java\", \"20000\"), (\"Python\", \"10000\"), (\"Scala\", \"5000\")]\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "# 1.1 Using toDF() function\n",
    "\n",
    "# Data Types are automatically infered from the data type of values, \n",
    "# otherwise we can supply schema to give data types manually.\n",
    "df = rdd.toDF()\n",
    "df = df.printSchema()\n",
    "\n",
    "df = rdd.toDF(columns)\n",
    "df.printSchema()\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 1.2 Using createDataFrame() function\n",
    "\n",
    "df = spark.createDataFrame(data).toDF(*columns)\n",
    "df.printSchema()\n",
    "\n",
    "# using Row type\n",
    "from pyspark.sql.types import Row\n",
    "rowData = map(lambda x: Row(*x), data)\n",
    "df = spark.createDataFrame(rowData, columns)\n",
    "df.show()\n",
    "\n",
    "# Define schema using StructType and StructField\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"M\",3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\",4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)\n",
    "  ]\n",
    "\n",
    "schema = StructType([ \n",
    "    StructField(\"firstname\",StringType(),True), \n",
    "    StructField(\"middlename\",StringType(),True), \n",
    "    StructField(\"lastname\",StringType(),True), \n",
    "    StructField(\"id\", StringType(), True), \n",
    "    StructField(\"gender\", StringType(), True), \n",
    "    StructField(\"salary\", IntegerType(), True) \n",
    "  ])\n",
    " \n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "#-------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a6d196",
   "metadata": {},
   "source": [
    "# 2. StructType and StructField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c06d9e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 2.1  Define nested StructType object struct\n",
    "\n",
    "data = [\n",
    "    ((\"James\",\"\",\"Smith\"),\"36636\",\"M\",3100),\n",
    "    ((\"Michael\",\"Rose\",\"\"),\"40288\",\"M\",4300),\n",
    "    ((\"Robert\",\"\",\"Williams\"),\"42114\",\"M\",1400),\n",
    "    ((\"Maria\",\"Anne\",\"Jones\"),\"39192\",\"F\",5500),\n",
    "    ((\"Jen\",\"Mary\",\"Brown\"),\"\",\"F\",-1)\n",
    "  ]\n",
    "\n",
    "schema = StructType([\n",
    "        StructField('name', StructType([\n",
    "             StructField('firstname', StringType(), True),\n",
    "             StructField('middlename', StringType(), True),\n",
    "             StructField('lastname', StringType(), True)\n",
    "             ])),\n",
    "         StructField('id', StringType(), True),\n",
    "         StructField('gender', StringType(), True),\n",
    "         StructField('salary', IntegerType(), True)\n",
    "         ])\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "df.printSchema()\n",
    "df.show()\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 2.2 Adding and changing the struct of the dataframe using struct() function\n",
    "\n",
    "df1 = df.withColumn(\"OtherInfo\",struct(\n",
    "                        col(\"id\").alias(\"identifier\"),\n",
    "                        col(\"gender\").alias(\"gender\"),\n",
    "                        col(\"salary\").alias(\"salary\"),\n",
    "                        when(col(\"salary\").cast(IntegerType()) < 2000, \"Low\")\n",
    "                        .when(col(\"salary\").cast(IntegerType()) < 4000, \"Medium\")\n",
    "                        .otherwise(\"High\").alias(\"salary_grade\")\n",
    "                            \n",
    "                    )).drop(\"id\", \"gender\", \"salary\")\n",
    "\n",
    "df1.printSchema()\n",
    "df1.show(truncate=False)\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 2.3 Using SQL ArrayType and MapType\n",
    "\n",
    "array_data = [\n",
    "    ((\"James\",\"\",\"Smith\"),[\"Cricket\", \"Football\", \"Chess\"], {\"eyecolor\" : \"blue\", \"tall\" : \"true\"}),\n",
    "    ((\"Michael\",\"Rose\",\"\"),[\"Cricket\", \"Football\", \"Chess\"], {\"eyecolor\" : \"blue\", \"tall\" : \"true\"}),\n",
    "    ((\"Robert\",\"\",\"Williams\"),[\"Cricket\", \"Football\", \"Chess\"], {\"eyecolor\" : \"blue\", \"tall\" : \"true\"}),\n",
    "    ((\"Maria\",\"Anne\",\"Jones\"),[\"Cricket\", \"Football\", \"Chess\"], {\"eyecolor\" : \"blue\", \"tall\" : \"true\"}),\n",
    "    ((\"Jen\",\"Mary\",\"Brown\"),[\"Cricket\", \"Football\", \"Chess\"], {\"eyecolor\" : \"blue\", \"tall\" : \"true\"})\n",
    "  ]\n",
    "\n",
    "\n",
    "array_structure_schema = StructType([\n",
    "    StructField(\"name\", StructType([\n",
    "        StructField(\"firstname\", StringType(), True),\n",
    "        StructField(\"middlename\", StringType(), True),\n",
    "        StructField(\"lastname\", StringType(), True)\n",
    "    ])),\n",
    "    StructField(\"hobbies\", ArrayType(StringType()), True),\n",
    "    StructField(\"properties\", MapType(StringType(), StringType(), True))\n",
    "])\n",
    "\n",
    "df2 = spark.createDataFrame(array_data, array_structure_schema)\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False)\n",
    "print(df2.schema.simpleString())\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 2.4 Creating StructType object from a JSON file\n",
    "\n",
    "\"\"\"\n",
    "import json\n",
    "\n",
    "schema_from_json = StructType.fromJson(json.loads(schema.json))\n",
    "df = spark.creatDataFrame(data, schema_from_json)\n",
    "\"\"\"\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 2.5 Creating schema using DDL string\n",
    "\n",
    "ddl_schema_str = \"\"\"\n",
    "                `fullname` STRUCT<`first`: STRING, `middle`: STRING, `last`: STRING>,\n",
    "                `age` INT,\n",
    "                `gender` STRING\n",
    "             \"\"\"\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 2.6 Checking if column exists in a dataframe\n",
    "\n",
    "# print(df.schema.fieldNames.contains(\"firstname\"))\n",
    "# print(df.schema.contains(StructField(\"firstname\", StringType, True)))\n",
    "\n",
    "#-------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a257df54",
   "metadata": {},
   "source": [
    "# 3. Row Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6447eb8d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------\n",
    "\n",
    "from pyspark.sql.types import Row\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# Key points for Row class\n",
    "\n",
    "# Earlier to Spark 3.0, when used Row class with named arguments, the fields are sorted by name.\n",
    "# Since 3.0, Rows created from named arguments are not sorted alphabetically instead they will be ordered \n",
    "# in the position entered.\n",
    "# To enable sorting by names, set the environment variable PYSPARK_ROW_FIELD_SORTING_ENABLED to true.\n",
    "# Row class provides a way to create a struct-type column as well.\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 3.1 Create a Row Object\n",
    "\n",
    "# Row class extends tuple class\n",
    "\n",
    "row = Row(\"James\", 40)\n",
    "print(f\"{row[0]} , {row[1]}\")\n",
    "\n",
    "# Row class accepts named arguments\n",
    "row = Row(name=\"Alice\", age=11)\n",
    "print(f\"{row.name}\")\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 3.2 Create custom class from Row\n",
    "\n",
    "Person = Row(\"name\", \"age\")\n",
    "p1 = Person(\"James\", 40)\n",
    "p2 = Person(\"Bob\", 50)\n",
    "print(f\"{p1.name}\")\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 3.3 Using Row class on RDD\n",
    "\n",
    "# When we use Row to create an RDD, after collecting the data we will get the result back in Row.\n",
    "\n",
    "data = [\n",
    "    Row(name=\"James,,Smith\", lang=[\"Java\", \"Scala\", \"C++\"], state=\"CA\"),\n",
    "    Row(name=\"Michael,,Rose\", lang=[\"Java\", \"Scala\", \"GoLang\"], state=\"NJ\"),\n",
    "    Row(name=\"Robert,,Williams\", lang=[\"Java\", \"CSharp\"], state=\"NV\")\n",
    "]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "print(rdd.collect())\n",
    "\n",
    "collected_data = rdd.collect()\n",
    "for row in collected_data:\n",
    "    print(f\"{row.name} , {row.lang}\")\n",
    "    \n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 3.4 Using Row class on DataFrame\n",
    "\n",
    "df = spark.createDataFrame(data) # column names are inferred from named arguments\n",
    "df.printSchema() \n",
    "df.show()\n",
    "\n",
    "df = spark.createDataFrame(data).toDF(*[\"languages_at_school\", \"name\", \"current_state\"])\n",
    "df.printSchema()\n",
    "df.show()\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 3.5 Create Nested Struct using Row class\n",
    "\n",
    "data = [\n",
    "    Row(name=\"James\", prop=Row(hair=\"black\", eye=\"blue\")),\n",
    "    Row(name=\"Ann\", prop=Row(hair=\"grey\", eye=\"black\"))\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "df.printSchema()\n",
    "df.show()\n",
    "\n",
    "#-------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa27469",
   "metadata": {},
   "source": [
    "# 4. Column Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fd47d6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------\n",
    "\n",
    "from pyspark.sql.types import Row\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 4.1 Create Column class object\n",
    "\n",
    "col_obj = lit(\"Hello World\")\n",
    "print(col_obj)\n",
    "\n",
    "data = [\n",
    "    (\"James\", 23),\n",
    "    (\"Bob\", 40)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data).toDF(\"name.fname\", \"age\")\n",
    "df.printSchema()\n",
    "\n",
    "# access column from dataframe\n",
    "\n",
    "# using df object\n",
    "\n",
    "df.select(df.age).show()\n",
    "df.select(df[\"age\"]).show()\n",
    "df.select(df[\"`name.fname`\"]).show()\n",
    "\n",
    "# using col function\n",
    "\n",
    "df.select(col(\"age\")).show()\n",
    "df.select(col(\"`name.fname`\")).show()\n",
    "\n",
    "# access struct type column values\n",
    "\n",
    "data = [\n",
    "    Row(name=\"James\", props=Row(eye=\"blue\", hair=\"black\")),\n",
    "    Row(name=\"Bob\", props=Row(eye=\"brown\", hair=\"white\"))\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "df.printSchema()\n",
    "\n",
    "df.select(df.props.hair).show()\n",
    "df.select(df[\"props.hair\"]).show()\n",
    "df.select(col(\"props.hair\")).show()\n",
    "df.select(col(\"props.*\")).show()\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 4.2 Column Operators\n",
    "\n",
    "data = [\n",
    "    (100, 2, 1),\n",
    "    (200, 3, 4),\n",
    "    (300, 4, 4)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data).toDF(\"col1\", \"col2\", \"col3\")\n",
    "\n",
    "# arithmetic operations\n",
    "df.select(df.col1 + df.col2, \n",
    "          df.col1 - df.col2, \n",
    "          df.col1 * df.col2, \n",
    "          df.col1 / df.col2, \n",
    "          df.col1 % df.col2).show()\n",
    "\n",
    "# logical comparison\n",
    "\n",
    "df.select((df.col2 > df.col3).alias(\"greater than comparison\"), df.col2 < df.col3, df.col2 == df.col3).show()\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 4.3 Column functions\n",
    "\n",
    "data = [\n",
    "    (\"James\", \"Bond\", \"100\", None),\n",
    "    (\"Bob\", \"Dylan\", \"200\", \"F\"),\n",
    "    (\"Tom Cruise\",\"XXX\", \"400\", \"\"),\n",
    "    (\"Tom Brand\", None, \"400\", 'M')\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data).toDF(\"fname\", \"lname\", \"id\", \"gender\")\n",
    "df.show()\n",
    "\n",
    "# alias()\n",
    "\n",
    "df.select(df.fname.alias(\"first_name\"), col(\"lname\").alias(\"last_name\"), expr(\"fname || ',' || lname\").alias(\"fullname\")).show()\n",
    "\n",
    "# sort(), asc() and desc() function\n",
    "\n",
    "df.sort(col(\"fname\").asc()).show()\n",
    "df.sort(df.fname.desc()).show()\n",
    "\n",
    "# cast() and astype() \n",
    "\n",
    "df.select(df.fname, df.id.cast(\"int\")).printSchema()\n",
    "\n",
    "# between()\n",
    "\n",
    "df.filter(df.id.between(100, 300)).show()\n",
    "\n",
    "# contains()\n",
    "\n",
    "df.filter(df.fname.contains(\"Cruise\")).show()\n",
    "\n",
    "# startswith() and endswith()\n",
    "\n",
    "df.filter(df.fname.startswith(\"T\")).show()\n",
    "df.filter(df.lname.endswith(\"Dylan\")).show()\n",
    "\n",
    "# isNUll() and isNotNull()\n",
    "\n",
    "df.filter(df.gender.isNull()).show()\n",
    "df.filter(df.lname.isNotNull()).show()\n",
    "\n",
    "# like() and rlike() \n",
    "\n",
    "df.select(col(\"fname\"), col(\"id\")).filter(col(\"fname\").like(\"%om%\")).show()\n",
    "\n",
    "# when() and otherwise()\n",
    "\n",
    "df.select(col(\"fname\"), \n",
    "          when(df.gender == 'M', \"Male\")\n",
    "          .when(df.gender == 'F', \"Female\")\n",
    "          .when(df.gender == None, '')\n",
    "          .otherwise(df.gender).alias(\"new_gender\")\n",
    "         ).show()\n",
    "\n",
    "# isin()\n",
    "\n",
    "li = [\"100\",  \"200\"]\n",
    "df.select(df.id).filter(df.id.isin(li)).show()\n",
    "\n",
    "# getField() and getItem()\n",
    "\n",
    "data = [\n",
    "    ((\"James\", \"Bond\"), [\"Java\", \"C++\"], {'hair':'black', 'eye':'brown'}),\n",
    "    ((\"Bob\", \"Dylan\"), [\"C#\", \"C++\"], {'hair':'white', 'eye':'blue'}),\n",
    "    ((\"John\", \"Doe\"), [\"Python\", \"C++\"], {'hair':'black', 'eye':'brown'}),\n",
    "    ((\"Mark\", \"Lee\"), [\"Scala\", \"C++\"], {'hair':'white', 'eye':'blue'})    \n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StructType([\n",
    "        StructField(\"fname\", StringType(), True),\n",
    "        StructField(\"lname\", StringType(), True)\n",
    "    ])),\n",
    "    StructField(\"languages\", ArrayType(StringType()), True),\n",
    "    StructField(\"props\", MapType(StringType(), StringType()), True)    \n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "df.printSchema()\n",
    "\n",
    "# getField()\n",
    "\n",
    "df.select(df.props.getField(\"hair\"), df.props.eye).show() # from Map\n",
    "df.select(col(\"name.fname\"), df.name.getField(\"lname\")).show() # from Struct\n",
    "\n",
    "# getItem()\n",
    "df.select(df.languages.getItem(1)).show()\n",
    "df.select(df.name.getItem(\"fname\")).show()\n",
    "\n",
    "#-------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b65ef5",
   "metadata": {},
   "source": [
    "# 5. Select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e191226",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------\n",
    "\n",
    "data = [\n",
    "    (\"Bob\", \"Dylan\", \"USA\", \"CA\"),\n",
    "    (\"John\", \"Smith\", \"USA\", \"NY\"),\n",
    "    (\"James\", \"Bond\", \"USA\", \"CA\"),\n",
    "    (\"Mark\", \"Lee\", \"USA\", \"FL\")\n",
    "]\n",
    "\n",
    "columns = [\"fname\", \"lname\", \"country\", \"state\"]\n",
    "\n",
    "df = spark.createDataFrame(data).toDF(*columns)\n",
    "df.show(truncate=False)\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 5.1 Select Single and Multiple Columns\n",
    "\n",
    "df.select(df.fname, \"lname\", col(\"country\"), df[\"state\"]).show()\n",
    "df.select(df.colRegex(\"`^.*name*`\")).show()\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 5.2 Select all columns from a List\n",
    "\n",
    "df.select(columns).show()\n",
    "df.select([col for col in df.columns]).show()\n",
    "df.select(\"*\").show()\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 5.3 Select columns by index\n",
    "\n",
    "df.select(df.columns[:3]).show()\n",
    "df.select(df.columns[2:4]).show()\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 5.4 Select Nested Struct columns\n",
    "\n",
    "\n",
    "data = [\n",
    "        ((\"James\",None,\"Smith\"),\"OH\",\"M\"),\n",
    "        ((\"Anna\",\"Rose\",\"\"),\"NY\",\"F\"),\n",
    "        ((\"Julia\",\"\",\"Williams\"),\"OH\",\"F\"),\n",
    "        ((\"Maria\",\"Anne\",\"Jones\"),\"NY\",\"M\"),\n",
    "        ((\"Jen\",\"Mary\",\"Brown\"),\"NY\",\"M\"),\n",
    "        ((\"Mike\",\"Mary\",\"Williams\"),\"OH\",\"M\")\n",
    "        ]\n",
    "       \n",
    "schema = StructType([\n",
    "    StructField('name', StructType([\n",
    "         StructField('firstname', StringType(), True),\n",
    "         StructField('middlename', StringType(), True),\n",
    "         StructField('lastname', StringType(), True)\n",
    "         ])),\n",
    "     StructField('state', StringType(), True),\n",
    "     StructField('gender', StringType(), True)\n",
    "     ])\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "df.select(df.name).show()\n",
    "df.select(df.name.firstname, col(\"name.lastname\")).show()\n",
    "df.select(col(\"name.*\")).show()\n",
    "\n",
    "#-------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205c0edc",
   "metadata": {},
   "source": [
    "# 6. Collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab79a7cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------\n",
    "\n",
    "dept = [\n",
    "    (\"Finance\", 10),\n",
    "    (\"Marketing\", 20),\n",
    "    (\"Sales\", 30),\n",
    "    (\"IT\", 40)\n",
    "]\n",
    "dept_cols = [\"dept_name\", \"dept_id\"]\n",
    "\n",
    "df = spark.createDataFrame(dept).toDF(*dept_cols)\n",
    "df.show()\n",
    "\n",
    "# collect all the data at driver node\n",
    "\n",
    "collected_data = df.collect()\n",
    "print(collected_data)\n",
    "\n",
    "for row in collected_data:\n",
    "    print(f\"{row.dept_name},{row.dept_id} || {row['dept_name']},{row['dept_id']} || {row[0]},{row[1]}\")\n",
    "        \n",
    "df.collect()[0][0] # get first row and value of first column\n",
    "\n",
    "collected_data = df.select(\"dept_name\").collect()\n",
    "print(collected_data)\n",
    "\n",
    "#-------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed46e0e6",
   "metadata": {},
   "source": [
    "# 7. withColumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a47c6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------\n",
    "\n",
    "data = [('James','','Smith','1991-04-01','M',3000),\n",
    "  ('Michael','Rose','','2000-05-19','M',4000),\n",
    "  ('Robert','','Williams','1978-09-05','M',4000),\n",
    "  ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
    "  ('Jen','Mary','Brown','1980-02-17','F',-1)\n",
    "]\n",
    "\n",
    "columns = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "df.printSchema()\n",
    "df.show()\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 7.1 change data type of a column\n",
    "\n",
    "df.withColumn(\"salary\", col(\"salary\").cast(IntegerType())).printSchema()\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 7.2 update the values of a column\n",
    "\n",
    "df.withColumn(\"salary\", expr(\"salary * 100\")).show()\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 7.3 create column from existing column\n",
    "\n",
    "df.withColumn(\"copied_column\", col(\"salary\") * -1).show()\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 7.4 add new column in a data frame\n",
    "\n",
    "df.withColumn(\"country\", lit(\"USA\")).withColumn(\"state\", lit(\"CA\")).show()\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 7.5 rename column\n",
    "\n",
    "df.withColumnRenamed(\"gender\", \"sex\").show()\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 7.6 drop column\n",
    "\n",
    "df.drop(\"salary\").show()\n",
    "\n",
    "#-------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7779bbeb",
   "metadata": {},
   "source": [
    "# 8. withColumnRenamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad99c98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------\n",
    "\n",
    "dataDF = [(('James','','Smith'),'1991-04-01','M',3000),\n",
    "  (('Michael','Rose',''),'2000-05-19','M',4000),\n",
    "  (('Robert','','Williams'),'1978-09-05','M',4000),\n",
    "  (('Maria','Anne','Jones'),'1967-12-01','F',4000),\n",
    "  (('Jen','Mary','Brown'),'1980-02-17','F',-1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "        StructField('name', StructType([\n",
    "             StructField('firstname', StringType(), True),\n",
    "             StructField('middlename', StringType(), True),\n",
    "             StructField('lastname', StringType(), True)\n",
    "             ])),\n",
    "         StructField('dob', StringType(), True),\n",
    "         StructField('gender', StringType(), True),\n",
    "         StructField('salary', IntegerType(), True)\n",
    "         ])\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(dataDF, schema)\n",
    "df.printSchema()\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 8.1 rename df column\n",
    "\n",
    "df.withColumnRenamed(\"dob\", \"date_of_birth\").printSchema()\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 8.2 rename multiple columns\n",
    "\n",
    "df.withColumnRenamed(\"dob\", \"date_of_birth\").withColumnRenamed(\"gender\", \"sex\").printSchema()\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 8.3 using StructType to rename nested columns\n",
    "\n",
    "schema2 = StructType([\n",
    "    StructField(\"fname\", StringType()),\n",
    "    StructField(\"middlename\", StringType()),\n",
    "    StructField(\"lname\", StringType())\n",
    "])\n",
    "\n",
    "df.withColumn(\"name\", col(\"name\").cast(schema2)).printSchema()\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 8.4 using select to rename nested elements\n",
    "\n",
    "df.select(col(\"name.firstname\").alias(\"fname\"), col(\"name.lastname\").alias(\"lname\")).printSchema()\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 8.5 using withColumn to rename nested columns\n",
    "\n",
    "df.withColumn(\"fname\", col(\"name.firstname\")).withColumn(\"lname\", col(\"name.lastname\")).printSchema()\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 8.6 using toDF() to change all column names in df\n",
    "\n",
    "new_cols = [\"col1\", \"col2\", \"col3\", \"col4\"]\n",
    "df.toDF(*new_cols).printSchema()\n",
    "\n",
    "#-------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a635643",
   "metadata": {},
   "source": [
    "# 9. where() and filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef1ea78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------\n",
    "\n",
    "data = [\n",
    "    ((\"James\",\"\",\"Smith\"),[\"Java\",\"Scala\",\"C++\"],\"OH\",\"M\"),\n",
    "    ((\"Anna\",\"Rose\",\"\"),[\"Spark\",\"Java\",\"C++\"],\"NY\",\"F\"),\n",
    "    ((\"Julia\",\"\",\"Williams\"),[\"CSharp\",\"VB\"],\"OH\",\"F\"),\n",
    "    ((\"Maria\",\"Anne\",\"Jones\"),[\"CSharp\",\"VB\"],\"NY\",\"M\"),\n",
    "    ((\"Jen\",\"Mary\",\"Brown\"),[\"CSharp\",\"VB\"],\"NY\",\"M\"),\n",
    "    ((\"Mike\",\"Mary\",\"Williams\"),[\"Python\",\"VB\"],\"OH\",\"M\")\n",
    " ]\n",
    "        \n",
    "schema = StructType([\n",
    "     StructField('name', StructType([\n",
    "        StructField('firstname', StringType(), True),\n",
    "        StructField('middlename', StringType(), True),\n",
    "         StructField('lastname', StringType(), True)\n",
    "     ])),\n",
    "     StructField('languages', ArrayType(StringType()), True),\n",
    "     StructField('state', StringType(), True),\n",
    "     StructField('gender', StringType(), True)\n",
    " ])\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 9.1 filter with column condition\n",
    "\n",
    "df.filter(df.state == 'OH').show()\n",
    "df.filter(df.state != 'OH').show()\n",
    "df.filter(~(df.state == 'OH')).show()\n",
    "df.filter(col(\"state\") == 'OH').show()\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 9.2 filter with SQL expression\n",
    "\n",
    "df.filter(\"gender == 'M'\").show()\n",
    "df.filter(\"gender != 'M'\").show()\n",
    "df.filter(\"gender <> 'M'\").show()\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 9.3 filter with multiple conditions\n",
    "\n",
    "df.filter((df.state == 'OH') & (df.gender=='M')).show()\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 9.4 filter based on list values\n",
    "\n",
    "li = ['OH', 'CA']\n",
    "\n",
    "df.filter(df.state.isin(li)).show()\n",
    "df.filter(~(df.state.isin(li))).show()\n",
    "df.filter(df.state.isin(li) == 'False').show()\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 9.5 filter based on startswith(), endswith(), contains()\n",
    "\n",
    "df.filter(col(\"state\").startswith(\"N\")).show()\n",
    "df.filter(df[\"state\"].endswith(\"Y\")).show()\n",
    "df.filter(df.state.contains(\"O\")).show()\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 9.6 filter on array column\n",
    "\n",
    "df.filter(array_contains(df.languages, \"Java\")).show()\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 9.7 filter on nested struct columns\n",
    "\n",
    "df.filter(df.name.lastname=='Williams').show(truncate=False)\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 9.8 like and rlike()\n",
    "\n",
    "data = [\n",
    "    (2,\"Shane Bond\"),\n",
    "    (3, \"James Bond\"),\n",
    "    (4, \"Mark bond\"),\n",
    "    (5, \"Brett Lee\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data=data).toDF(\"id\", \"name\")\n",
    "df.show()\n",
    "\n",
    "# case sensitive comparison\n",
    "\n",
    "df.filter(df.name.like(\"%bond%\")).show()\n",
    "\n",
    "# case in-sensitive comparison using rlike(regex like)\n",
    "\n",
    "df.filter(df.name.rlike(\"(?i)^*bond\")).show()\n",
    "\n",
    "#-------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b8c223",
   "metadata": {},
   "source": [
    "# 10.  distinct(), drop() and dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ce569e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------\n",
    "\n",
    "data = [\n",
    "        (\"James\", \"Sales\", 3000), \n",
    "        (\"Michael\", \"Sales\", 4600), \n",
    "        (\"Robert\", \"Sales\", 4100),\n",
    "        (\"Maria\", \"Finance\", 3000),\n",
    "        (\"James\", \"Sales\", 3000), \n",
    "        (\"Scott\", \"Finance\", 3300),\n",
    "        (\"Jen\", \"Finance\", 3900),\n",
    "        (\"Jeff\", \"Marketing\", 3000),\n",
    "        (\"Kumar\", \"Marketing\", 2000),\n",
    "        (\"Saif\", \"Sales\", 4100),\n",
    "]\n",
    "\n",
    "columns= [\"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "df.printSchema()\n",
    "df.orderBy(\"department\", \"salary\").show(truncate=False)\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 10.1 get distinct rows by comparing all columns using distict() function\n",
    "\n",
    "df1 = df.distinct()\n",
    "df1.orderBy(\"department\", \"salary\").show()\n",
    "\n",
    "# 10.2 get distinct rows by using dropDuplicates() function\n",
    "df1 = df.dropDuplicates()\n",
    "df1.orderBy(col(\"department\"), col(\"salary\")).show()\n",
    "\n",
    "# 10.3 get distinct rows considering few columns by using dropDuplicates() function\n",
    "df1 = df.dropDuplicates([\"department\", \"salary\"])\n",
    "df1.orderBy(df1.department, df1.salary).show()\n",
    "\n",
    "#-------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d1a65e",
   "metadata": {},
   "source": [
    "# 11. orderBy() and sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a8517d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------\n",
    "\n",
    "\"\"\"\n",
    "We can use either sort() or orderBy() function\n",
    "to sort df by ascending or descending order\n",
    "based on single or multiple columns.\n",
    "\"\"\"\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "simpleData = [\n",
    "    (\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000),\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
    "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000),\n",
    "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000),\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)\n",
    "  ]\n",
    "columns= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "df = spark.createDataFrame(data=simpleData, schema=columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 11.1 sort() function\n",
    "\n",
    "df.sort(\"department\", \"state\").show()\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 11.2 orderBy() function\n",
    "\n",
    "df.orderBy(col(\"department\"), col(\"state\")).show()\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 11.3 asc() and desc() methods\n",
    "\n",
    "df.sort(col(\"department\").asc(), col(\"state\").desc()).show()\n",
    "df.orderBy(df.department.asc(), df.state.desc()).show()\n",
    "\n",
    "#-------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb376284",
   "metadata": {},
   "source": [
    "# 12. groupBy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3be34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------\n",
    "\n",
    "simpleData = [\n",
    "    (\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000),\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
    "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000),\n",
    "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000),\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)\n",
    "  ]\n",
    "columns= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "df = spark.createDataFrame(data=simpleData, schema=columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "df1 = df.groupBy(\"department\").agg(\n",
    "    sum(\"salary\").alias(\"total_salary\"), avg(\"salary\").alias(\"average_salary\")\n",
    ")\n",
    "df1.show()\n",
    "\n",
    "df1 = df.groupBy(\"department\").max(\"salary\")\n",
    "df1.show()\n",
    "\n",
    "df1 = df.groupBy(\"department\").count()\n",
    "df1.show()\n",
    "\n",
    "df1 = df.groupBy(\"department\", \"state\").sum(\"salary\", \"bonus\")\n",
    "df1.show()\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# filter on aggregate data\n",
    "\n",
    "df1 = df.groupBy(\"department\").agg(\n",
    "    sum(\"salary\").alias(\"total_salary\")\n",
    ").where(col(\"total_salary\") >= 200000)\n",
    "\n",
    "df1.show()\n",
    "\n",
    "#-------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42e56e1",
   "metadata": {},
   "source": [
    "# 13. Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ce9368",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------\n",
    "\n",
    "emp = [\n",
    "    (1,\"Smith\",-1,\"2018\",\"10\",\"M\",3000),\n",
    "    (2,\"Rose\",1,\"2010\",\"20\",\"M\",4000),\n",
    "    (3,\"Williams\",1,\"2010\",\"10\",\"M\",1000),\n",
    "    (4,\"Jones\",2,\"2005\",\"10\",\"F\",2000),\n",
    "    (5,\"Brown\",2,\"2010\",\"40\",\"\",-1),\n",
    "    (6,\"Brown\",2,\"2010\",\"50\",\"\",-1)\n",
    "]\n",
    "\n",
    "empColumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\",\"emp_dept_id\",\"gender\",\"salary\"]\n",
    "\n",
    "empDF = spark.createDataFrame(data=emp, schema=empColumns)\n",
    "empDF.printSchema()\n",
    "empDF.show(truncate=False)\n",
    "\n",
    "dept = [\n",
    "    (\"Finance\",10),\n",
    "    (\"Marketing\",20), \n",
    "    (\"Sales\",30), \n",
    "    (\"IT\",40) \n",
    "]\n",
    "\n",
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "deptDF = spark.createDataFrame(data=dept, schema=deptColumns)\n",
    "deptDF.printSchema()\n",
    "deptDF.show(truncate=False)\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# inner, left, right, full, leftsemi, leftanti\n",
    "\n",
    "df = empDF.join(deptDF, empDF.emp_dept_id == deptDF.dept_id, \"inner\")\n",
    "df.show()\n",
    "\n",
    "# self join\n",
    "\n",
    "df = empDF.alias(\"emp1\").join(empDF.alias(\"emp2\"), col(\"emp1.superior_emp_id\") == col(\"emp2.emp_id\") ,\"left\").select(\n",
    "    col(\"emp1.emp_id\"), col(\"emp1.name\"), col(\"emp2.emp_id\").alias(\"superior_emp_id\"), col(\"emp2.name\").alias(\"manager_name\"))\n",
    "df.show()\n",
    "\n",
    "#-------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9238cb",
   "metadata": {},
   "source": [
    "# 14. union() and unionAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f09966b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------\n",
    "\n",
    "simpleData = [\n",
    "    (\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000),\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000)\n",
    "  ]\n",
    "\n",
    "columns= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "df1 = spark.createDataFrame(data=simpleData, schema=columns)\n",
    "df1.printSchema()\n",
    "df1.show(truncate=False)\n",
    "\n",
    "simpleData2 = [\n",
    "    (\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000),\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)\n",
    "  ]\n",
    "columns2= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "df2 = spark.createDataFrame(data=simpleData2, schema=columns2)\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False)\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "df1.union(df2).show()\n",
    "df1.unionAll(df2).show()\n",
    "df1.union(df2).distinct().show()\n",
    "\n",
    "#-------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4cb7da",
   "metadata": {},
   "source": [
    "# 15. UDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12c2423",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------\n",
    "\n",
    "columns = [\"Seqno\",\"Name\"]\n",
    "\n",
    "data = [\n",
    "    (\"1\", \"john jones\"),\n",
    "    (\"2\", \"tracey smith\"),\n",
    "    (\"3\", \"amy sanders\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "\n",
    "df.show(truncate=False)\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 15.1 create udf and use it as column object expression\n",
    "\n",
    "def convert_case(name):\n",
    "    res = \"\"\n",
    "    words = name.split()\n",
    "    for word in words:\n",
    "        res = res + word[0].upper() + word[1:] + \" \"\n",
    "    return res\n",
    "\n",
    "convert_case_udf = udf(convert_case, StringType())\n",
    "\n",
    "df1 = df.withColumn(\"Name\", convert_case_udf(col(\"Name\")))\n",
    "df1.show()\n",
    "\n",
    "df2 = df.select(\"Seqno\", convert_case_udf(col(\"Name\")).alias(\"Name\"))\n",
    "df2.show()\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 15.2 create udf and use it as SQL function\n",
    "\n",
    "spark.udf.register(\"convert_case_udf\", convert_case, StringType())\n",
    "\n",
    "df.createOrReplaceTempView(\"student_tbl\")\n",
    "\n",
    "df3 = spark.sql(\"select Seqno, convert_case_udf(Name) as `Name` from student_tbl\")\n",
    "df3.show()\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 15.3 creating udf using annotation\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def upper_case(string):\n",
    "    return string.upper()\n",
    "\n",
    "df.withColumn(\"Name\", upper_case(col(\"Name\"))).show()\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 15.4 Null check\n",
    "\n",
    "def capitalize(string):\n",
    "    \"\"\"\n",
    "    Whenever possible handle null \n",
    "    check inside the function \n",
    "    implementation\n",
    "    \"\"\"\n",
    "    if bool(string):\n",
    "        return string.capitalize()\n",
    "    return \"\"\n",
    "\n",
    "cols = [\n",
    "    \"Seqno\",\n",
    "    \"Name\"\n",
    "]\n",
    "\n",
    "data = [\n",
    "    (\"1\", \"john jones\"),\n",
    "    (\"2\", \"tracey smith\"),\n",
    "    (\"3\", \"amy sanders\"),\n",
    "    ('4',None)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=cols)\n",
    "df.show()\n",
    "\n",
    "capitalize_udf = udf(capitalize, StringType())\n",
    "\n",
    "df.withColumn(\"Name\", capitalize_udf(col(\"Name\"))).show()\n",
    "\n",
    "#-------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e40bf12",
   "metadata": {},
   "source": [
    "# 16. map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c4303c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------\n",
    "\n",
    "\"\"\"\n",
    "1. map() transformation can be only applied on RDD's.\n",
    "Hence to apply map transformation on DataFrame, convert\n",
    "it into RDD and then apply transformation.\n",
    "\"\"\"\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 16.1 map() on rdd \n",
    "\n",
    "data = [\n",
    "    \"Project\",\n",
    "    \"Gutenberg’s\",\n",
    "    \"Alice’s\",\n",
    "    \"Adventures\",\n",
    "    \"in\",\n",
    "    \"Wonderland\",\n",
    "    \"Project\",\n",
    "    \"Gutenberg’s\",\n",
    "    \"Adventures\",\n",
    "    \"in\",\n",
    "    \"Wonderland\",\n",
    "    \"Project\",\n",
    "    \"Gutenberg’s\"\n",
    "]\n",
    "\n",
    "word_rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "paired_rdd = word_rdd.map(lambda x: (x,1))\n",
    "\n",
    "for el in paired_rdd.collect():\n",
    "    print(el)\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 16.2 map() on DataFrame\n",
    "\n",
    "\n",
    "data = [\n",
    "  ('James','Smith','M',30),\n",
    "  ('Anna','Rose','F',41),\n",
    "  ('Robert','Williams','M',62), \n",
    "]\n",
    "\n",
    "columns = [\"firstname\",\"lastname\",\"gender\",\"salary\"]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "df.show()\n",
    "\n",
    "# convert df to rdd to apply map transformation\n",
    "rdd2 = df.rdd.map(lambda x: (x.firstname + \",\" + x.lastname, x.gender, x.salary * 3))\n",
    "df2 = rdd2.toDF([\"name\", \"gender\", \"salary\"])\n",
    "df2.show()\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Different ways to refer columns inside a map function.\n",
    "\n",
    "1. using index:\n",
    "rdd2 = df.rdd.map(lambda x: (x[0] + \",\" + x[1], x[2], x[3] * 2))\n",
    "\n",
    "2. using column names:\n",
    "rdd2 = df.rdd.map(lambda x: (x[\"firstname\"] + \",\" + x[\"lastname\"], x[\"gender\"], x[\"salary\"] * 2))\n",
    "rdd2 = df.rdd.map(lambda x: (x.firstname + \",\" + x.lastname, x.gender, x.salary * 2))\n",
    "\n",
    "3. create a custom function to perform an operation:\n",
    "def func(x):\n",
    "    fname = x.firstname\n",
    "    lname = x.lastname\n",
    "    gender = x.gender.lower()\n",
    "    sal = x.salary * 2\n",
    "    return (fname + ',' + lastname, gender, sal)\n",
    "    \n",
    "rdd2 = df.rdd.map(lamda x: func(x))\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#-------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171c76af",
   "metadata": {},
   "source": [
    "# 17. flatMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f1d69e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 17.1 flatMap() on rdd\n",
    "\n",
    "data = [\n",
    "    \"Project Gutenberg’s\",\n",
    "    \"Alice’s Adventures in Wonderland\",\n",
    "    \"Project Gutenberg’s\",\n",
    "    \"Adventures in Wonderland\",\n",
    "    \"Project Gutenberg’s\"\n",
    "]\n",
    "\n",
    "rdd=spark.sparkContext.parallelize(data)\n",
    "for el in rdd.collect():\n",
    "    print(el)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "rdd2 = rdd.flatMap(lambda x: x.split())\n",
    "for el in rdd2.collect():\n",
    "    print(el)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "paired_rdd = rdd2.map(lambda x: (x,1))\n",
    "\n",
    "grouped_rdd = paired_rdd.groupByKey().mapValues(lambda x: len(x))\n",
    "for el in grouped_rdd.collect():\n",
    "    print(el)\n",
    "    \n",
    "print(\"\\n\")\n",
    "\n",
    "reduced_by_key_rdd = paired_rdd.reduceByKey(lambda x,y: x+y)\n",
    "for el in reduced_by_key_rdd.collect():\n",
    "    print(el)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 17.2 flatMap() on DataFrame\n",
    "\n",
    "arrayData = [\n",
    "    ('James',['Java','Scala'],{'hair':'black','eye':'brown'}),\n",
    "    ('Michael',['Spark','Java',None],{'hair':'brown','eye':None}),\n",
    "    ('Robert',['CSharp',''],{'hair':'red','eye':''}),\n",
    "    ('Washington',None,None),\n",
    "    ('Jefferson',['1','2'],{})\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data=arrayData, schema = ['name','knownLanguages','properties'])\n",
    "\n",
    "df1 = df.select(\"name\", explode(col(\"knownLanguages\")).alias(\"lang\"))\n",
    "df1.show()\n",
    "\n",
    "df2 = df.select(\"name\", explode(col(\"properties\")).alias(\"part\", \"color\"))\n",
    "df2.show()\n",
    "\n",
    "\n",
    "rdd1 = df.rdd.flatMap(lambda x: x.knownLanguages if bool(x.knownLanguages) else [\"unknown\"]) # rdd of type str\n",
    "rdd1 = rdd1.map(lambda x: (x,))  # or rdd1.map(lambda x: Row(x))\n",
    "df3 = rdd1.toDF([\"lang\"])\n",
    "df3.show()\n",
    "\n",
    "#-------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e2ea89",
   "metadata": {},
   "source": [
    "# 18. foreach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26a57a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------\n",
    "\n",
    "columns = [\"Seqno\",\"Name\"]\n",
    "\n",
    "data = [\n",
    "    (\"1\", \"john jones\"),\n",
    "    (\"2\", \"tracey smith\"),\n",
    "    (\"3\", \"amy sanders\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "df.show(truncate=False)\n",
    "\n",
    "df.foreach(lambda x: print(f\"{x.Seqno} --> {x.Name}\"))\n",
    "\n",
    "def func(x):\n",
    "    print(x.Name.upper())\n",
    "    \n",
    "df.foreach(func)\n",
    "\n",
    "#-------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10d4862",
   "metadata": {},
   "source": [
    "# 19. fillna() and fill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8c968c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "This functions are used to replace \n",
    "null or None values with:\n",
    "1. zero\n",
    "2. empty string\n",
    "3. space\n",
    "4. constant literal\n",
    "\n",
    "fillna() and fill()\n",
    "are aliases of each\n",
    "other and returns the\n",
    "same results.\n",
    "\"\"\"\n",
    "\n",
    "data = [\n",
    "    (1,704,'STANDARD',None,'PR',30100),\n",
    "    (2,704,None,'PASEO COSTA DEL SUR','PR',None),\n",
    "    (3,709,None,'BDA SAN LUIS','PR',3700),\n",
    "    (4,76166,'UNIQUE','CINGULAR WIRELESS','TX',84000),\n",
    "    (5,76177,'STANDARD',None,'TX',None)\n",
    "]\n",
    "\n",
    "cols = ['id', 'zipcode', 'type', 'city', 'state', 'population']\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=cols)\n",
    "df.printSchema()\n",
    "df.show()\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 19.1 replace null or None values with 0 for int and long type cols\n",
    "\n",
    "df.na.fill(value=0).show()\n",
    "df.na.fill(value=0,subset=['population']).show()\n",
    "df.fillna(value=0).show()\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 19.2 replace null or None values with empty string for string type cols\n",
    "\n",
    "df.na.fill(value='').show()\n",
    "df.na.fill(value='unknown', subset=['city']).na.fill(value='', subset=['type']).show()\n",
    "df.na.fill(value={'city': 'unknown', 'type': ''}).show()\n",
    "\n",
    "#-------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef54f31f",
   "metadata": {},
   "source": [
    "# 20. pivot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ad40dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------\n",
    "\n",
    "data = [\n",
    "    ('apple', 1000, 'USA'),\n",
    "    ('carrot', 1500, 'USA'),\n",
    "    ('beans', 1600, 'USA'),\n",
    "    ('orange', 2000, 'USA'),\n",
    "    ('orange', 2000, 'USA'),\n",
    "    ('apple', 400, 'CHINA'),\n",
    "    ('carrot', 1200, 'CHINA'),\n",
    "    ('beans', 1500, 'CHINA'),\n",
    "    ('orange', 4000, 'CHINA'),\n",
    "    ('apple', 2000, 'CANADA'),\n",
    "    ('carrot', 2000, 'CANADA'),\n",
    "    ('beans', 2000, 'MEXICO')\n",
    "]\n",
    "\n",
    "cols = ['product', 'amount', 'country']\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=cols)\n",
    "df.printSchema()\n",
    "df.show()\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 20.1 pivot spark dataframe\n",
    "\n",
    "pivot_df = df.groupBy('product').pivot('country').agg(sum(\"amount\").alias(\"total_amount\"))\n",
    "pivot_df.show()\n",
    "\n",
    "# pivot() is an expensive operation. PySpark 2.0 onwards its performance has been improved.\n",
    "# It uses two phase aggregation to improve the performance\n",
    "\n",
    "pivot_df = (df\n",
    "            .groupBy('product', 'country')\n",
    "            .agg(sum('amount').alias('total_amount'))\n",
    "            .groupBy('product')\n",
    "            .pivot('country')\n",
    "            .agg(sum('total_amount'))\n",
    "           )\n",
    "pivot_df.show()\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 20.2 unpivot spark dataframe\n",
    "\n",
    "unpivot_df = pivot_df.select('product', \n",
    "                             expr(\"stack(4,'CANADA',CANADA,'CHINA',CHINA,'MEXICO',MEXICO,'USA',USA) as (country, total_amount)\")\n",
    "                            )\n",
    "unpivot_df = unpivot_df.filter(col(\"total_amount\").isNotNull()).orderBy(\"country\")\n",
    "unpivot_df.show()\n",
    "\n",
    "#-------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d372cd7",
   "metadata": {},
   "source": [
    "# 21.ArrayType()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7744d511",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------\n",
    "\n",
    "data = [\n",
    "    (\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],[\"Spark\",\"Java\"],\"OH\",\"CA\"),\n",
    "    (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],[\"Spark\",\"Java\"],\"NY\",\"NJ\"),\n",
    "    (\"Robert,,Williams\",[\"CSharp\",\"VB\"],[\"Spark\",\"Python\"],\"UT\",\"NV\")\n",
    "]\n",
    "\n",
    "schema = StructType([ \n",
    "    StructField(\"name\",StringType(),True), \n",
    "    StructField(\"languagesAtSchool\",ArrayType(StringType()),True), \n",
    "    StructField(\"languagesAtWork\",ArrayType(StringType()),True), \n",
    "    StructField(\"currentState\", StringType(), True), \n",
    "    StructField(\"previousState\", StringType(), True)\n",
    "  ])\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "df.printSchema()\n",
    "df.show()\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 21.1 explode()\n",
    "\n",
    "df.select(df.name, explode(df.languagesAtSchool)).show()\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 21.2 split()\n",
    "\n",
    "df.select(split(df.name, ',').alias(\"nameAsArray\")).show() \n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 21.3 array()\n",
    "\n",
    "\"\"\"\n",
    "use array() function to create a new array column\n",
    "by merging data from multiple columns.\n",
    "All input columns must have same data type.\n",
    "\"\"\"\n",
    "\n",
    "df.select(df.name, array(df.currentState, df.previousState).alias(\"states\")).show()\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 21.4 array_contains()\n",
    "\n",
    "df.select(df.name, array_contains(df.languagesAtSchool, \"Java\")).show()\n",
    "\n",
    "#-------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5abdad6",
   "metadata": {},
   "source": [
    "# 22. MapType()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ecea13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------\n",
    "\n",
    "dataDictionary = [\n",
    "    ('James',{'hair':'black','eye':'brown'}),\n",
    "    ('Michael',{'hair':'brown','eye':None}),\n",
    "    ('Robert',{'hair':'red','eye':'black'}),\n",
    "    ('Washington',{'hair':'grey','eye':'grey'}),\n",
    "    ('Jefferson',{'hair':'brown','eye':''})\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('name', StringType(), True),\n",
    "    StructField('properties', MapType(StringType(),StringType()),True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data=dataDictionary, schema = schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 22.1 extract key and value into separate columns\n",
    "\n",
    "df1 = df.rdd.map(lambda x: (x.name, x.properties[\"eye\"], x.properties[\"hair\"])).toDF([\"name\", \"eye\", \"hair\"])\n",
    "df1.show()\n",
    "\n",
    "df1 = df.withColumn(\"eye\", df.properties.getItem(\"eye\")).withColumn(\"hair\", df.properties.getItem(\"hair\"))\n",
    "df1.show()\n",
    "\n",
    "df1 = df.withColumn(\"eye\", df.properties[\"eye\"]).withColumn(\"hair\",df.properties[\"hair\"])\n",
    "df1.show()\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 22.2 explode()\n",
    "\n",
    "df.select(df.name, explode(df.properties)).show()\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 22.3 map_keys()\n",
    "\n",
    "df.select(df.name, map_keys(df.properties)).show()\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# 22.4 map_values()\n",
    "\n",
    "df.select(df.name, map_values(df.properties)).show()\n",
    "\n",
    "#-------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
